---
title: "Model evaluation"
author: "Ich"
date: "3/9/2019"
output: html_document
params:
  readonly: TRUE
editor_options: 
  chunk_output_type: console
---



```{r knitr-setup}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp =  0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)

```


# Test of params

```{r eval = params$readonly}

cat("Readlonly is ")

params$readonly
```





# Load packages


```{r}
library(sjmisc)
library(viridis)
library(pradadata)  # elec data
library(tidyverse)
library(knitr)
library(tidylog)
library(gghighlight)

library(rprojroot)

library(rethinking)  # Bayes modeling
library("rstanarm")
```




## Other setup


```{r}
proj_root <- rprojroot::find_root(as.root_criterion(is_rstudio_project))
```


## Change options


```{rset-options}
theme_set(theme_classic())

options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill = "viridis")

scale_colour_discrete <- scale_colour_viridis_d
scale_fill_discrete <- scale_fill_viridis_d

```




# Data

## Load data


```{r load-data}
load(paste0(proj_root, "/objects/models.Rda"))

d79 <- read.csv(paste0(proj_root, "/objects/d79.csv"))
d <- read_csv(paste0(proj_root, "/objects/d_prepared.csv"))
```


Model descritpion file:


```{r}
model_description <- read_csv2(paste0(proj_root, "/objects/model-description.csv"))
```


```{r}
best_model_precis_raw <- precis(models[[best_model_name]], depth = 2)@output %>% 
  rownames_to_column(var = "Coefficient")
```



## Define labels

```{r best-model-precis}
best_model_precis <- best_model_precis_raw %>% 
  mutate(state_id = parse_number(str_extract(Coefficient, "\\[\\d+\\]"))) %>% 
  full_join(state_dict) %>% 
  mutate(coeff_label = state)

best_model_precis
```


Check labels:

```{r}
d79$state_id %>% unique()
d79$state %>% unique()
```



# Setup





## Check model object

```{r}
names(models)
```


# Model comparison

## Compare null models

```{r compare-null-models}
compare_null_models <- compare( models[["m0a"]], models[["m0b"]], 
                                models[["m0c"]], models[["m0d"]])
compare_null_models
```


## Compare models of reduced sample size


### Define model vector



Here are only the Cauchy models:



```{r}
names(models)

model_names <- names(models)


```


### Compare the models 





Only models with Cauchy Sigmas:

```{r}
compare_reduced_sample <- compare(models[["m0b"]],
                                  models[["m1b"]],
                                  models[["m2b"]],
                                  models[["m3b"]],
                                  models[["m4b"]],
                                  models[["m5b"]],
                                  models[["m6b"]],
                                  models[["m7b"]],
                                  models[["m8b"]],
                                  models[["m9b"]],
                                  models[["m10c"]],
                                  models[["m6d"]],
                                  models[["m0d"]],
                                  models[["m0e"]],
                                  models[["m3d"]],
                                  models[["m11d"]],
                                  models[["m12d"]])

model_comparison <- compare_reduced_sample@output %>% 
  rownames_to_column() %>% 
  as_tibble() %>% 
  mutate(model = str_extract(rowname, "m\\d\\w+")) %>% 
  select(-rowname) %>% 
  left_join(model_description, by = c("model" = "name_old")) %>% 
  filter(included == "yes", `outcome variable` %in% c( "afd_prop_log", "afd_prop")) %>% 
#  select(name, multilevel, k, predictors, `outcome distribution`, WAIC, SE, weight) %>% 
  arrange(WAIC)

model_comparison %>% kable()


```


### Vector of count models


```{r}
vector_count_model_names <- model_comparison %>% 
  filter(`outcome distribution` == "Poisson") %>% 
  pull(model)
```




### Plot comparison

```{r}
plot(compare_reduced_sample, se = TRUE) %>% str()
```




### Save comparison to disk
```{r}
write_csv(model_comparison,
          path = paste0(proj_root, "/objects/model_comparison.csv"))
```



# Best model


## Best bunch of best models (top 5)

```{r}
best_name_vector <- model_comparison %>% 
  arrange(WAIC) %>% 
  slice(1:5) %>% 
  pull(model)
  
best_name_vector

```


```{r}
models_short <- models[best_name_vector]
names(models_short)
names(models)

models79 <- models_short
```



## Best models including count

```{r}
best_name_vector_incl_count <- c(best_name_vector, "m12d", "m9b", "m0e")



models_short_incl_count <- models[best_name_vector_incl_count]
names(models_short_incl_count)
names(models)

models79_incl_count <- models_short_incl_count

```



## Get best model 

Which model ist best? Let's define the model with the lowest WAIC as best:

```{r}
compare_reduced_sample@output %>% 
  rownames_to_column(var = "model_name") %>%  
  slice(which.min(WAIC)) -> best_model

best_model
```


And now extract the *name* of the best model:

```{r}
best_model_name_long <-
  best_model %>%
  pull(model_name)

best_model_name <- str_extract(best_model_name_long, "\\w\\d{1,2}\\w")


best_model_name
```




## Describe best model

```{r}


write.csv(best_model_precis,
          file = paste0(proj_root, "/objects/best_model_precis.csv"))
```


## Save traceplot of most favorable model


```{r}
my_traceplot(models[[best_model_name]])
```

Save by hand!


## State names 

Wait, what's the state names associated to the betas?



```{r}
d79 %>% 
  distinct(state)
```

Hm, is this really the right order?

What about this one?

```{r}
levels(factor(d79$state))
```

I assume this one makes more sense.




# Computing prediction errors



## Helper function

Here's a function to compute the modeling error, defined as the absolute difference of the estimated model value (of afd proportion) minus the observed value (of afd proportion).


```{r fun-comp-err}
comp_abs_error <- function(model, data, fun = mean) {
  posterior_per_person <- link(model)
  
  
  as_tibble(posterior_per_person) %>% 
    summarise_all(fun) %>% 
    gather() %>% 
    rename(estimate = value) %>% 
    mutate(afd_prop_log = data$afd_prop_log,
           error = abs(estimate - afd_prop_log)) %>% 
    pull(error) -> error_vec
  
  return(error_vec)
}


```





Apply the function on all models:

```{r comp-all-model-errors, results = "hide"}
model_error <- purrr::map(models79, comp_abs_error, data = d79)

names(model_error) <- names(models79)
```


```{r}
model_error_long <- model_error %>% 
  as_tibble() %>% 
  gather(key = model, value = abs_error) %>% 
  mutate(is_best_model = ifelse(model == best_model_name, 
                                TRUE, 
                                FALSE))
```


### Count 

Now with the count models:

```{r comp-all-model-errors-incl-count, results = "hide"}
model_error_incl_count <- purrr::map(models79_incl_count, comp_abs_error, data = d79)

names(model_error) <- names(models79)
```


```{r}
model_error_long_incl_count <- model_error_incl_count %>% 
  as_tibble() %>% 
  gather(key = model, value = abs_error) %>% 
  mutate(is_best_model = ifelse(model == best_model_name, 
                                TRUE, 
                                FALSE))


```



## Plot the distribution of absolute MEAN errors per model

```{r plot-err-distrib}
 

p_mod_err_density <- model_error_long %>% 
  ggplot(aes(x = abs_error)) +
  facet_grid(. ~ model) +
  geom_density(aes(fill = is_best_model)) +
  theme_classic() +
  labs(title = "Distribution of absolute prediction error per model",
       caption = "Note. Target variable is log. of AfD proportion")
p_mod_err_density

ggsave(filename = paste0(proj_root, "/img/p_mod_err_density.png"),
       width = 8, height = 4)




p_mod_err_boxplots <- model_error_long %>% 
  ggplot(aes(x = model, y= abs_error)) +
  theme_classic() +
  geom_boxplot(aes(fill = is_best_model))  +
  labs(title = "Distribution of absolute prediction error per model",
       caption = "Note. Target variable is log. of AfD proportion",
       y = "Absolute error")
p_mod_err_boxplots

ggsave(filename = paste0(proj_root, "/img/p_mod_err_boxplots.png"),
       width = 8, height = 4)
  
```


### Count

Count only, beacsue in count the error is on the raw number of obervations.

Get best Count model:

```{r}
model_error_long_incl_count <-  model_error_long_incl_count %>% 
  dplyr::filter(model %in% vector_count_model_names) %>% 
  mutate(is_best_model = abs_error == min(.$abs_error))

```


```{r}
p_mod_err_boxplots_incl_count <- model_error_long_incl_count %>% 
  ggplot(aes(x = model, y= abs_error)) +
  theme_classic() +
  geom_boxplot()  +
  labs(title = "Distribution of absolute prediction error per model",
       caption = "Note. Target variable is count of AfD votes",
       y = "Absolute error")
p_mod_err_boxplots_incl_count

ggsave(plot = p_mod_err_boxplots_incl_count,
       filename = paste0(proj_root, "/img/p_mod_err_boxplots_incl_count.png"),
       width = 8, height = 4)
  

```



## Mean error of best model

Add the MEAN error of THE BEST MODEL to the data frame:

```{r}
d79 <- d79 %>% 
  mutate(err_avg = model_error[[best_model_name]])
```


Let's look at the distribution:

```{r}
d79 %>% 
  ggplot(aes(x = err_avg, y = ..density..)) +
  geom_histogram() +
  geom_density()  +
  labs(title = "Distribution of absolute prediction error of best performing model",
       caption = "Note. Target variable is log. of AfD proportion ")
```



## Abolute error per model

Compute the median absolute error:


```{r md-abs-err-md, results = "hide"}
md_abs_error_all_models <- map_dbl(model_error, median)
md_abs_error_all_models
```

The median absolute errors show the averaged absolute error (as per median) of for each predicted case.



Compute the *mean* absolute error:


```{r md-abs-err-mean, results = "hide"}
mean_abs_error_all_models <- map_dbl(model_error, mean) 
mean_abs_error_all_models
```

The median absolute errors show the averaged absolute error (as per median) of for each predicted case.



Also, compute the IQR of the errors:

```{r iqr-abs-err, results = "hide"}
modell_error_IQR <- lapply(models79, comp_abs_error, fun = IQR, data = d79)

md_of_model_iqr_error <- sapply(modell_error_IQR, median) %>% unlist()
```




# Average prediction

## Compute average prediction



```{r fun-comp-avg-pred}
comp_avg_pred <- function(model, data, fun = mean) {
  
  posterior_per_person <- link(model)
  
  
  as_tibble(posterior_per_person) %>% 
    summarise_all(fun) %>% 
    gather() %>% 
    rename(estimate = value) %>% 
    pull(estimate) -> predictions_vec
  
  return(predictions_vec)
}


```


Apply the function on all models:

```{r comp-all-model-avg-preds, results = "hide"}
model_avg_predictions <- lapply(models79, comp_avg_pred, data = d79)

names(model_avg_predictions) <- names(models79)
```


## Mean prediction of best model

Add the MEAN error of THE BEST MODEL to the data frame:

```{r}
d79 <- d79 %>% 
  mutate(pred_avg = model_avg_predictions[[best_model_name]])
```




## R squared


### R squared of best model, based on AVERAGE error

Attention this is the uncorrected overfitting prone R squared. 
Note ther's also no information about variability (ie, only point estimate).


```{r}
my_r2 <- function(error, observed = d79$afd_prop_log) {
  
  output <- 1 - var(error)/var(observed)
}

r2_bestmodel <- my_r2(error = d79$err_avg, observed = d79$afd_prop_log)

r2_bestmodel

```





### R squared function

Input: 
 - samples from the posterior (eg, 1000)
 - outcome variable
 
 Output:
 - Distribution of R squared samples
 
 
 
### Helper function




```{r}
comp_err <- function(estimated, observed) {
  output = estimated - observed
}



comp_rsquared <- function(model, observed_data = d79$afd_prop_log)  {
  
  posterior_per_person <- link(model) %>% t()
  
  r2_samples <- posterior_per_person %>% 
    as_tibble() %>% 
    transmute_all(list(error_value = ~ comp_err(estimated = ., observed = observed_data))) %>% 
    summarise_all(my_r2) %>% 
    gather(key = sample, value = r2) 
}


names(models79)

```


Apply the function on all models:

```{r}
r2_all_models <- models79 %>% 
  purrr::map(comp_rsquared) %>% 
  set_names(names(models79))


```




Convert it to a long data frame, and rememeber the names of the models:


```{r}
r2_all_models_df <- r2_all_models %>% 
  map_dfr("r2") %>% 
  gather(key = model, value = r2)

r2_all_models_df %>% 
  head()



```


Find highest median:

```{r}
r2_md <- r2_all_models_df %>% 
  group_by(model) %>% 
  summarise(md_r2 = median(r2)) %>% 
  mutate(highest_median = md_r2 == max(md_r2))



highest_r2_md <-r2_md$model[r2_md$highest_median == TRUE]

```



Now plot it:


```{r}
p_r2_all_models <- r2_all_models_df %>% 
  mutate(hightes_r2_md = model == highest_r2_md) %>% 
  ggplot(aes(x = model, y = r2, fill = hightes_r2_md)) +
  geom_boxplot() +
  labs(y = expression(paste("R"^2))) +
  theme_classic()

p_r2_all_models

ggsave(filename = paste0(proj_root, "/img/p_r2_all_models.png"),
       width = 8, height = 4)
```

 

### Compute the R^2



Apply the function on all models:

```{r comp-rsquared-all-models, results = "hide"}
model_error <- purrr::map(models79, comp_abs_error, data = d79)

names(model_error) <- names(models79)
```


# Visualizing prediction error

Some preparation:

```{r prepare-error-data}
model_error %>% 
    as.data.frame() -> model_error_df


names(model_error_df) <- names(models79)

model_error_df %>% 
  mutate(afd_prop_log = d79$afd_prop_log,
         id = 1:nrow(model_error_df)) -> model_error_df


modell_error_IQR %>% 
  as.data.frame() -> model_error_IQR_df


names(model_error_IQR_df) <- names(models79)

model_error_IQR_df %>% 
  mutate(id = 1:nrow(model_error_IQR_df)) -> model_error_IQR_df

```



Convert to long version for plotting:

```{r convert-error-data}
model_error_IQR_df %>% 
  gather(key = model, value = iqr, -c(id)) %>% 
  mutate(stat = "IQR") -> model_error_IQR_df_long


model_error_df %>% 
  gather(key = model, value = error, -c(afd_prop_log, id)) %>% 
  mutate(stat = "median") -> model_error_df_long

model_error_df_long %>% 
  bind_rows(model_error_IQR_df_long) -> model_error_long


model_error_df_long %>% 
  left_join(model_error_IQR_df_long, by = c("id", "model")) %>% 
  select(-c(stat.x, stat.y)) -> model_error_md_iqr

glimpse(model_error_md_iqr)

```


Now plot:

```{r plot-model-error, out.width = "100%"}


as_tibble(md_abs_error_all_models) %>% 
  mutate(model = best_name_vector,
         best_model = ifelse(model == best_model_name, TRUE, FALSE)) -> md_abs_error_all_models

md_abs_error_all_models

glimpse(md_abs_error_all_models)

p_model_error_md_iqr <- model_error_md_iqr %>% 
  arrange(-error) %>% 
  ggplot(aes(x = id)) +
  facet_wrap(~model) +
  geom_hline(aes(yintercept = value,
                 color = best_model), 
             size = 3,
             data = md_abs_error_all_models
             ) +
  geom_errorbar(aes(ymin = error - (iqr/2),
                    ymax = error + (iqr/2)),
                alpha = .3,
                color = "gray40") +
  geom_point(aes(y = error), alpha = .1) +
  geom_label(aes(label = round(value, 3)), x = 1, y = .2, 
            data = md_abs_error_all_models, 
            hjust = 0) +
  guides(color=FALSE) +
  theme_classic() +
  labs(x = "ID of electorial district",
       y = "(absolute) prediction error") 
  
p_model_error_md_iqr


ggsave(filename = paste0(proj_root, "/img/model-error-comp.png"),
       width = 8, height = 4)
ggsave(filename = paste0(proj_root, "/img/model-error-comp.pdf"))
```






# Plotting prediction error against observed values


```{r p-obs-est-err-values, out.width="100%"}
posterior_per_person_best_model <- link(models[[best_model_name]])

d79$area_nr <- as.integer(d79$area_nr)
  
posterior_per_person_best_model %>%  
  as_tibble() %>% 
    summarise_all(median, na.rm = TRUE) %>% 
    gather() %>% 
    rename(estimate = value) %>% 
  cbind(d79) %>% 
  mutate(error = abs(estimate - afd_prop_log),
         top05 = percent_rank(error) >= .95) %>% 
  drop_na(estimate) -> d_short_w_pred_err 

dim(d_short_w_pred_err)


polygon_pos <- data.frame(
  x = c(-3, 1, 1,    -3, 1, -3, -3 ),
  y = c(-3, -3, 1,      -3, 1, 1, -3),
  value = c("underestimates", "underestimates", "underestimates", "overestimates", "overestimates", "overestimates", "overestimates")
)
 
sample_size <- nrow(d79)

p_d_short_w_pred_err <- d_short_w_pred_err %>%  
  ggplot() +
  aes(x = afd_prop_log, y = estimate) +
  geom_abline(slope = 1, intercept = 0, color = "grey60") +
  geom_polygon(data = polygon_pos, aes(x = x, y = y, fill = value), alpha = .1) +
  geom_point(data = filter(d_short_w_pred_err, top05 == TRUE),
             color = "grey40",
             alpha = .6,
             size = 7) +
  geom_point(aes(color = error,
                 shape = top05),
             alpha = .8,
             size = 3) +
  ggrepel::geom_label_repel(aes(label = area_name), data = filter(d_short_w_pred_err, top05 == TRUE)) +
  annotate("text", x = 1, y = -3, label = "model understimates", hjust = 1, vjust = 0) +
  annotate("text", x = -3, y = 1, label = "model overestimates", hjust = 0, vjust = 1) +
  labs(x = "Log. of observed AfD votes",
       y = "Log. of estimated AfD votes",
       title = "Modelled vs. observed AFD votes. Top 5 percent predicted errors are labelled",
       caption = paste0("n = ", sample_size, " electoral districts; data provided by Bundeswahlleiter 2017")) +
  guides(fill = FALSE) +
  theme_classic()

p_d_short_w_pred_err


ggsave(paste0(proj_root, "/img/modelest-vs-obs.pdf", width = 10, height = 5))

ggsave(plot = p_d_short_w_pred_err,
       filename = paste0(proj_root, "/img/modelest-vs-obs.png"),
                         width = 10, height = 5)
```






# Model coefficients plot of most fav. model


## Level 1

```{r}


p_best_model_precis_level1 <- best_model_precis %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  dplyr::filter(Coefficient %in% c("sigma", "beta1", "beta2")) %>%  
  ggplot(aes(y = Coefficient)) +
  geom_errorbarh(aes(xmin = `lower 0.89`, xmax = `upper 0.89`), height = .1) +
  geom_point(aes(x = Mean), color = "red", size = 2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "grey60") +
  theme_classic() +
  labs(x = "beta weight",
       y = "Coefficient",
       caption = "Note. Error bars indicate 89% percentile intervals. Red dots refer to the mean.") 

p_best_model_precis_level1


ggsave(plot = p_best_model_precis_level1,
       filename = paste0(proj_root, "/img/p_best_model_precis_level1.png"),
       height = 4, width = 8)


```


## Level 2

Now level 2:

Average slope between states:

```{r}
mean_slope_between_states <- best_model_precis %>% 
  dplyr::filter(str_detect(Coefficient, "beta0")) %>% 
  summarise(beta_states_mean = mean(Mean, na.rm = TRUE)) %>% 
  pull(beta_states_mean)
```


```{r fav-model-l2}

p_best_model_precis_level2 <- best_model_precis %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  dplyr::filter(str_detect(Coefficient, "beta0")) %>%  
  ggplot(aes(y = coeff_label)) +
  geom_errorbarh(aes(xmin = `lower 0.89`, xmax = `upper 0.89`), height = .1) +
  geom_point(aes(x = Mean), color = "red", size = 2) +
  geom_vline(aes(xintercept = mean_slope_between_states), linetype = "dashed", color = "grey60") +
  theme_classic() +
  labs(x = "beta weight (log of AfD votes)",
       y = "State coefficient",
       caption = "Note. Error bars indicate 89% percentile intervals. Red dots refer to the mean. The dashed line shows the mean of the state slopes.") 

p_best_model_precis_level2


ggsave(plot = p_best_model_precis_level2,
       filename = paste0(proj_root, "/img/p_best_model_precis_level2.png"),
       height = 4, width = 8)

```


Now the de-logged values:




```{r fav-model-l2}

p_best_model_precis_level2_delogged <- best_model_precis %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  dplyr::filter(str_detect(Coefficient, "beta0")) %>%  
  mutate(mean_delogged = exp(Mean),
         low89 = exp(`lower 0.89`),
         up89 = exp(`upper 0.89`)) %>% 
  ggplot(aes(y = coeff_label)) +
  geom_errorbarh(aes(xmin = low89, xmax = up89), height = .1) +
  geom_point(aes(x = mean_delogged), color = "red", size = 2) +
  geom_vline(aes(xintercept = mean(mean_delogged)), linetype = "dashed", color = "grey60") +
  theme_classic() +
  labs(x = "beta weight (AfD votes proportion)",
       y = "State coefficient",
       caption = "Note. Error bars indicate 89% percentile intervals. Red dots refer to the mean. The dashed line shows the mean of the state slopes.") 

p_best_model_precis_level2_delogged


ggsave(plot = p_best_model_precis_level2_dleogged,
       filename = paste0(proj_root, "/img/p_best_model_precis_level2_delogged.png"),
       height = 4, width = 8)

```



# Check posterior distribution of predictors

Let's have a look at the posterior distribution of the `best_model`.

```{r}
post_best_model <- extract.samples(models[[best_model_name]])
```


This object is a list. Let's convert it to a data frame for easier plotting.

```{r}
post_best_model_df <- tibble(
  sigma = post_best_model[["sigma"]],
  sigma2 = post_best_model[["sigma2"]],
  beta1 = post_best_model[["beta1"]],
  beta2= post_best_model[["beta2"]]
)
```

And now we plot a number of histograms:

```{r}
post_best_model_df <- post_best_model_df %>% 
  gather() %>%  
  rename(coef = key) %>% 
  mutate(coef = factor(coef, labels = c("beta1: foreigners rate", 
                                        "beta2: unemployment rate",
                                        "sigma: sd of outcome variable prior",
                                        "sigma2: sd of fedederal state prior")))
  
  
post_best_model_df %>% 
  ggplot() +
  aes(x = value) +
  facet_wrap(~coef, scales = "free") +
  theme_classic() + 
  geom_histogram() -> p_post_best_model

p_post_best_model
```


Now compute summary statistics:

```{r}
post_best_model_df %>% 
  group_by(coef) %>% 
  summarise(q05 = quantile(value, .05),
            q50 = quantile(value, .5),
            q95 = quantile(value, .95),
            value = mean(value)
  )  -> post_best_model_df_sum
  #gather(key = my_quantile, value = value, -coef) -> post_best_model_df_sum

head(post_best_model_df_sum)
```


Now plot both:

```{r}
p_post_best_model +
  geom_rect(data = post_best_model_df_sum,
              aes(xmin = q05,
                  xmax = q95,
                  ymin = 0,
                  ymax = Inf),
            fill = "red",
            alpha = .2) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y = element_blank()) +
  theme_classic() + 
  labs(
       y = "",
       caption = "Note. Shaded areas demark 90% mass intervals")

ggsave(paste0(proj_root, "/img/p-post-best-model.pdf"))
ggsave(paste0(proj_root, "/img/p-post-best-model.png"),
       width = 8, height = 4)
```








# Geo plotting


AfD success in the election:


```{r afd-geoplot}
wahlkreise_shp %>% 
  left_join(select(d, area_nr, afd_prop), by = c("WKR_NR" = "area_nr")) %>% 
  ggplot() +
  geom_sf(aes(fill = afd_prop)) +
  theme_void() +
  scale_fill_viridis() +
  labs(fill="Afd votes\n(Zweitstimme)",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_afd
p_afd

ggsave(paste0(proj_root, "/img/p-afd.pdf"))

ggsave(paste0(proj_root, "/img/p-afd.png"),
       width = 8, height = 4)

```


Unemployment rates in Germany per district:


```{r unemp-geoplot}
wahlkreise_shp %>% 
  left_join(select(d, area_nr, unemp_n, total_n), by = c("WKR_NR" = "area_nr")) %>% 
  mutate(unemp_prop = unemp_n / total_n) %>% 
  ggplot() +
  geom_sf(aes(fill = unemp_prop)) + 
  theme_void() +
  scale_fill_viridis() +
  labs(fill="unemployment rate",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_unemp
p_unemp

# 
ggsave(paste0(proj_root, "/img/p-unemp.pdf")
ggsave(paste0(proj_root, "/img/p-unemp.png"),
       width = 8, height = 4)       
```


Foreigner rates:

```{r for-geoplot}
wahlkreise_shp %>% 
  left_join(select(d, area_nr, for_prop, total_n), by = c("WKR_NR" = "area_nr")) %>% 
  ggplot() +
  geom_sf(aes(fill = for_prop)) + 
  theme_void() +
  scale_fill_viridis() +
  labs(fill="Foreigner rate",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_foreign

p_foreign

# ggsave(paste0(proj_root, "/img/p-foreigners.pdf")
ggsave(paste0(proj_root, "/img/p-foreign.png"),
       width = 8, height = 4)
```


Joint diagrams

```{r}
library(gridExtra)

grid.arrange(p_unemp, p_afd, nrow = 1)
grid.arrange(p_foreign, p_afd, nrow = 1)
```





## Plot prediction errors



```{r pred-error-geoplot}
wahlkreise_shp %>% 
  left_join(select(d79, area_nr, err_avg), by = c("WKR_NR" = "area_nr")) %>% 
  #drop_na(afd_prop) %>% 
  ggplot() +
  geom_sf(aes(fill = err_avg)) + 
  theme_void() +
  scale_fill_viridis() +
  labs(title = "Average mean prediction errors of best model",
       fill = "Average absolute \nprediction error",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_pred_error

p_pred_error

ggsave(paste0(proj_root, "/img/p_pred_error.pdf"))
ggsave(plot = p_pred_error,
       filename = paste0(proj_root, "/img/p_pred_error.png"),
       width = 5, height = 5)
```











# Check linearity assumption



Let's compute the predictions for each model:



```{r fun-comp-preds}

model_predictions <- lapply(models79, link)
 
```


Now plot the predictions against the error, as advised by Gelman and Hill.

First, get predictions of the best moel:

```{r}
model_predictions[[best_model_name]] %>% 
  as_tibble %>% 
  summarise_all(mean) %>% 
  gather() -> best_model_preds

```

Each observation is one *row*  in this data frame.

Similarly, get errors of the best model:


```{r}
model_error[[best_model_name]] %>% 
  as_tibble() %>% 
  rename(error = value) %>% 
  mutate(pred = best_model_preds$value) -> best_model_pred_err

p_best_model_pred_err <- best_model_pred_err %>% 
  ggplot() +
  aes(x = pred, y = error) +
  geom_hline(yintercept = quantile(best_model_pred_err$error, .5),
             color = "grey60") +
  geom_hline(yintercept = quantile(best_model_pred_err$error, .975),
             color = "grey80", , linetype = "dashed") +
  geom_hline(yintercept = quantile(best_model_pred_err$error, .025),
             color = "grey80", , linetype = "dashed") +
  geom_point() +
  theme_classic() +
  labs(title = best_model_name,
       xlab = "Model predictions",
       ylab = "Model error",
       caption = "Note. Horizontal lines denote .025, .5, and .975 quantiles, respectively") 

p_best_model_pred_err 

 
ggsave(p_best_model_pred_err ,
       paste0(proj_root, "/img/p_best_model_pred_err.pdf"))
ggsave(plot = p_best_model_pred_err ,
       filename = paste0(proj_root, "/img/p_best_model_pred_err.png"), width = 8, height = 4)
```





# SessionInfo

```{r}
sessionInfo()
```







# Save results


```{r eval = params$readonly}
save(models, 
     file = paste0(proj_root, "/objects/models.Rda"))

save(compare_null_models,
     file = paste0(proj_root, "/objects/compare_null_models.Rda"))

save(compare_reduced_sample,
     file = paste0(proj_root, "/objects/compare_reduced_sample.Rda"))

save(model_error, 
     file = paste0(proj_root, "/objects/model_error.Rda"))

save(modell_error_IQR, 
     file = paste0(proj_root, "/objects/modell_error_IQR.Rda"))


save(model_error_df, 
     file = paste0(proj_root, "/objects/model_error_df.RDa"))

save(model_error_IQR_df, 
     file = paste0(proj_root, "/objects/model_error_IQR_df.RDa"))


save(model_error_md_iqr, 
     file = paste0(proj_root, "/objects/model_error_md_iqr.RDa"))


save(best_model_preds, 
     file = paste0(proj_root, "/objects/best_model_preds.Rda"))

save(model_predictions,
     file = paste0(proj_root, "/objects/model_predictions.Rda"))


save(model_avg_predictions,
     file = paste0(proj_root, "objects/model_avg_predictions.Rda"))


save(best_model_precis,
     file = paste0(proj_root, "/objects/best_model_prcis.Rda"))



```

