---
title: "Model evaluation"
author: "Ich"
date: "3/9/2019"
output: html_document
params:
  readonly: TRUE
editor_options: 
  chunk_output_type: console
---



```{r knitr-setup}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp =  0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)

```


# Test of params

```{r eval = params$readonly}

cat("Readlonly is ")

params$readonly
```





# Load packages


```{r}
library(sjmisc)
library(viridis)
library(pradadata)  # elec data
library(tidyverse)
library(knitr)
library(tidylog)
library(gghighlight)

library(rprojroot)

library(rethinking)  # Bayes modeling
library("rstanarm")
```




## Other setup


```{r}
proj_root <- rprojroot::find_root(as.root_criterion(is_rstudio_project))
```


## Change options


```{rset-options}
theme_set(theme_classic())

options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill = "viridis")

scale_colour_discrete <- scale_colour_viridis_d
scale_fill_discrete <- scale_fill_viridis_d

```




# Data

## Load data


```{r load-data}
load(paste0(proj_root, "/objects/models.Rda"))

d79 <- read.csv(paste0(proj_root, "/objects/d79.csv"))
d <- read_csv(paste0(proj_root, "/objects/d_prepared.csv"))
```


# Setup





## Check model object

```{r}
names(models)
```


# Model comparison

## Compare null models

```{r compare-null-models}
compare_null_models <- compare( models[["m0a"]], models[["m0b"]], models[["m0c"]])
compare_null_models
```


## Compare models of reduced sample size


### Define model vector



Here are only the Cauchy models:



```{r}

models79 <- models[c("m0b",
                     "m1b",
                     "m2b",
                     "m3b",
                     "m4b",
                     "m5b",
                     "m6b")]

length(models79)

names(models79)
names(models)

model_names <- names(models79)

```


### Compare the models 


Only models with Cauchy Sigmas:

```{r}
compare_reduced_sample <- compare(models[["m0b"]],
                                  models[["m1b"]],
                                  models[["m2b"]],
                                  models[["m3b"]],
                                  models[["m4b"]],
                                  models[["m5b"]],
                                  models[["m6b"]],
                                  models[["m7b"]],
                                  models[["m8b"]])

model_comparison <- compare_reduced_sample@output %>% 
  rownames_to_column() %>% 
  as_tibble() %>% 
  mutate(model = str_extract(rowname, "m\\d.")) %>% 
  select(-rowname)

model_comparison %>% kable()
```




### Plot comparison

```{r}
plot(compare_reduced_sample, se = TRUE) %>% str()
```




### Save comparison to disk
```{r}
write_csv(model_comparison,
          path = paste0(proj_root, "/objects/model_comparison.csv"))
```



# Best model


## Get best model 

Which model ist best? Let's define the model with the lowest WAIC as best:

```{r}
compare_reduced_sample@output %>% 
  rownames_to_column(var = "model_name") %>%  
  slice(which.min(WAIC)) -> best_model

best_model
```


And now extract the *name* of the best model:

```{r}
best_model_name <-
  best_model %>%
  pull(model_name)

best_model_name

if (best_model_name == 'models[[\"m4a\"]]') best_model_name = "m4a"
if (best_model_name == 'models[[\"m4b\"]]') best_model_name = "m4b"

best_model_name
```




## Describe best model

```{r}
best_model_precis <- precis(models[[best_model_name]], depth = 2)@output

write.csv(best_model_precis,
          file = paste0(proj_root, "/objects/best_model_precis.csv"))
```


## Save traceplot of most favorable model


```{r}
my_traceplot(models[[best_model_name]])
```


## State names 

Wait, what's the state names associated to the betas?



```{r}
d79 %>% 
  distinct(state)
```

Hm, is this really the right order?

What about this one?

```{r}
levels(factor(d79$state))
```

I assume this one makes more sense.




# Computing prediction errors



## Helper function

Here's a function to compute the modeling error, defined as the absolute difference of the estimated model value (of afd proportion) minus the observed value (of afd proportion).


```{r fun-comp-err}
comp_abs_error <- function(model, data, fun = mean) {
  posterior_per_person <- link(model)
  
  
  as_tibble(posterior_per_person) %>% 
    summarise_all(fun) %>% 
    gather() %>% 
    rename(estimate = value) %>% 
    mutate(afd_prop_log_z = data$afd_prop_log_z,
           error = abs(estimate - afd_prop_log_z)) %>% 
    pull(error) -> error_vec
  
  return(error_vec)
}

```





Apply the function on all models:

```{r comp-all-model-errors, results = "hide"}
model_error <- purrr::map(models79, comp_abs_error, data = d79)

names(model_error) <- names(models79)
```


## Plot the distribution of absolute MEAN errors per model

```{r plot-err-distrib}
model_error_long <- model_error %>% 
  as_tibble() %>% 
  gather(key = model, value = abs_error) %>% 
  mutate(is_best_model = ifelse(model == best_model_name, 
                                TRUE, 
                                FALSE)) 

p_mod_err_density <- model_error_long %>% 
  ggplot(aes(x = abs_error)) +
  facet_grid(. ~ model) +
  geom_density(aes(fill = is_best_model)) +
  theme_classic() +
  labs(title = "Distribution of absolute prediction error per model",
       caption = "Note. Target variable is log. of AfD proportion (z-standardized)")
p_mod_err_density

ggsave(filename = paste0(proj_root, "/img/p_mod_err_density.png"),
       width = 8, height = 4)




p_mod_err_boxplots <- model_error_long %>% 
  ggplot(aes(x = model, y= abs_error)) +
  theme_classic() +
  geom_boxplot(aes(fill = is_best_model))  +
  labs(title = "Distribution of absolute prediction error per model",
       caption = "Note. Target variable is log. of AfD proportion (z-standardized)",
       y = "Absolute error")
p_mod_err_boxplots

ggsave(filename = paste0(proj_root, "/img/p_mod_err_boxplots.png"),
       width = 8, height = 4)
  
```



## Mean error of best model

Add the MEAN error of THE BEST MODEL to the data frame:

```{r}
d79 <- d79 %>% 
  mutate(err_avg = model_error[[best_model_name]])
```


Let's look at the distribution:

```{r}
d79 %>% 
  ggplot(aes(x = err_avg, y = ..density..)) +
  geom_histogram() +
  geom_density()  +
  labs(title = "Distribution of absolute prediction error of best performing model",
       caption = "Note. Target variable is log. of AfD proportion (z-standardized)")
```



## Abolute error per model

Compute the median absolute error:


```{r md-abs-err-md, results = "hide"}
md_abs_error_all_models <- map_dbl(model_error, median)
md_abs_error_all_models
```

The median absolute errors show the averaged absolute error (as per median) of for each predicted case.



Compute the *mean* absolute error:


```{r md-abs-err-mean, results = "hide"}
mean_abs_error_all_models <- map_dbl(model_error, mean) 
mean_abs_error_all_models
```

The median absolute errors show the averaged absolute error (as per median) of for each predicted case.



Also, compute the IQR of the errors:

```{r iqr-abs-err, results = "hide"}
modell_error_IQR <- lapply(models79, comp_abs_error, fun = IQR, data = d79)

md_of_model_iqr_error <- sapply(modell_error_IQR, median) %>% unlist()
```




# Average prediction

## Compute average prediction



```{r fun-comp-avg-pred}
comp_avg_pred <- function(model, data, fun = mean) {
  
  posterior_per_person <- link(model)
  
  
  as_tibble(posterior_per_person) %>% 
    summarise_all(fun) %>% 
    gather() %>% 
    rename(estimate = value) %>% 
    pull(estimate) -> predictions_vec
  
  return(predictions_vec)
}

```


Apply the function on all models:

```{r comp-all-model-avg-preds, results = "hide"}
model_avg_predictions <- lapply(models79, comp_avg_pred, data = d79)

names(model_avg_predictions) <- names(models79)
```


## Mean prediction of best model

Add the MEAN error of THE BEST MODEL to the data frame:

```{r}
d79 <- d79 %>% 
  mutate(pred_avg = model_avg_predictions[[best_model_name]])
```




## R squared


### R squared of best model, based on AVERAGE error

Attention this is the uncorrected overfitting prone R squared. 
Note ther's also no information about variability (ie, only point estimate).


```{r}
my_r2 <- function(error, observed = d79$afd_prop_log_z) {
  
  output <- 1 - var(error) / var(observed)
}

r2_bestmodel <- my_r2(error = d79$err_avg, observed = d79$afd_prop_log_z)

r2_bestmodel

```





### R squared function

Input: 
 - samples from the posterior (eg, 1000)
 - outcome variable
 
 Output:
 - Distribution of R squared samples
 
 
 
### Helper function




```{r}
comp_err <- function(estimated, observed) {
  output = estimated - observed
}



comp_rsquared <- function(model, observed_data = d79$afd_prop_log_z)  {
  
  posterior_per_person <- link(model) %>% t()
  
  r2_samples <- posterior_per_person %>% 
    as_tibble() %>% 
    transmute_all(funs(error_value = comp_err(estimated = ., observed = observed_data))) %>% 
    summarise_all(my_r2) %>% 
    gather(key = sample, value = r2) 
}

```


Apply the function on all models:

```{r}
r2_all_models <- models79 %>% 
  map(comp_rsquared) %>% 
  set_names(names(models79))
```



Convert it to a long data frame, and rememeber the names of the models:


```{r}
r2_all_models_df <- r2_all_models %>% 
  map_dfr("r2") %>% 
  gather(key = model, value = r2)
```


Find highest median:

```{r}
r2_md <- r2_all_models_df %>% 
  group_by(model) %>% 
  summarise(md_r2 = median(r2)) %>% 
  mutate(highest_median = md_r2 == max(md_r2))



highest_r2_md <-r2_md$model[r2_md$highest_median == TRUE]

```



Now plot it:


```{r}
p_r2_all_models <- r2_all_models_df %>% 
  mutate(hightes_r2_md = model == highest_r2_md) %>% 
  ggplot(aes(x = model, y = r2, fill = hightes_r2_md)) +
  geom_boxplot() +
  labs(y = "R squared") +
  theme_classic()

p_r2_all_models

ggsave(filename = paste0(proj_root, "/img/p_r2_all_models.png"),
       width = 8, height = 4)
```

 

### Compute the R^2



Apply the function on all models:

```{r comp-rsquared-all-models, results = "hide"}
model_error <- purrr::map(models79, comp_abs_error, data = d79)

names(model_error) <- names(models79)
```


# Visualizing prediction error

Some preparation:

```{r prepare-error-data}
model_error %>% 
    as.data.frame() -> model_error_df


names(model_error_df) <- names(models79)

model_error_df %>% 
  mutate(afd_prop_log_z = d79$afd_prop_log_z,
         id = 1:nrow(model_error_df)) -> model_error_df


modell_error_IQR %>% 
  as.data.frame() -> model_error_IQR_df


names(model_error_IQR_df) <- names(models79)

model_error_IQR_df %>% 
  mutate(id = 1:nrow(model_error_IQR_df)) -> model_error_IQR_df

```



Convert to long version for plotting:

```{r convert-error-data}
model_error_IQR_df %>% 
  gather(key = model, value = iqr, -c(id)) %>% 
  mutate(stat = "IQR") -> model_error_IQR_df_long


model_error_df %>% 
  gather(key = model, value = error, -c(afd_prop_log_z, id)) %>% 
  mutate(stat = "median") -> model_error_df_long

model_error_df_long %>% 
  bind_rows(model_error_IQR_df_long) -> model_error_long


model_error_df_long %>% 
  left_join(model_error_IQR_df_long, by = c("id", "model")) %>% 
  select(-c(stat.x, stat.y)) -> model_error_md_iqr

glimpse(model_error_md_iqr)

```


Now plot:

```{r plot-model-error, out.width = "100%"}


as_tibble(md_abs_error_all_models) %>% 
  mutate(model = model_names,
         best_model = ifelse(model_names == best_model_name, TRUE, FALSE)) -> md_abs_error_all_models

glimpse(md_abs_error_all_models)

p_model_error_md_iqr <- model_error_md_iqr %>% 
  arrange(-error) %>% 
  ggplot(aes(x = id)) +
  facet_wrap(~model) +
  geom_hline(aes(yintercept = value,
                 color = best_model), 
             size = 3,
             data = md_abs_error_all_models
             ) +
  geom_errorbar(aes(ymin = error - (iqr/2),
                    ymax = error + (iqr/2)),
                alpha = .3,
                color = "gray40") +
  geom_point(aes(y = error), alpha = .1) +
  geom_label(aes(label = round(value, 3)), x = 1, y = .2, 
            data = md_abs_error_all_models, 
            hjust = 0) +
  guides(color=FALSE) +
  theme_classic() +
  labs(x = "ID of electorial district",
       y = "(absolute) prediction error") 
  
p_model_error_md_iqr


ggsave(filename = paste0(proj_root, "/img/model-error-comp.png"),
       width = 8, height = 4)
ggsave(filename = paste0(proj_root, "/img/model-error-comp.pdf"))
```






# Plotting prediction error against observed values


```{r p-obs-est-err-values, out.width="100%"}
posterior_per_person_best_model <- link(models[[best_model_name]])
  
posterior_per_person_best_model %>%  
  as_tibble() %>% 
    summarise_all(median) %>% 
    gather() %>% 
    rename(estimate = value) %>% 
  add_column(area_nr = 1:nrow(.)) %>% 
  full_join(d79) %>%
  mutate(error = abs(estimate - afd_prop_log_z),
         top05 = percent_rank(error) >= .95) %>% 
  drop_na(estimate) -> d_short_w_pred_err 

dim(d_short_w_pred_err)


polygon_pos <- data.frame(
  x = c(-2, 2, 2,    -2, 2, -2, -2 ),
  y = c(-2, -2, 2,      -2, 2, 2, -2),
  value = c("underestimates", "underestimates", "underestimates", "overestimates", "overestimates", "overestimates", "overestimates")
)
 
p_d_short_w_pred_err <- d_short_w_pred_err %>%  
  ggplot() +
  aes(x = afd_prop_log_z, y = estimate) +
  geom_abline(slope = 1, intercept = 0, color = "grey60") +
  geom_polygon(data = polygon_pos, aes(x = x, y = y, fill = value), alpha = .1) +
  geom_point(data = filter(d_short_w_pred_err, top05 == TRUE),
             color = "grey40",
             alpha = .6,
             size = 7) +
  geom_point(aes(color = error,
                 shape = top05),
             alpha = .8,
             size = 3) +
  ggrepel::geom_label_repel(aes(label = area_name), data = filter(d_short_w_pred_err, top05 == TRUE)) +
  annotate("text", x = 2, y = -2, label = "model understimates", hjust = 1, vjust = 0) +
  annotate("text", x = -2, y = 2, label = "model overestimates", hjust = 0, vjust = 1) +
  labs(x = "Log. of observed AfD votes (z-standardized)",
       y = "Log. of estimated AfD votes (standardized)",
       title = "Modelled vs. observed AFD votes. Top 5 percent predicted errors are labelled",
       caption = "n=79 electoral districts; data provided by Bundeswahlleiter 2017") +
  guides(fill = FALSE) +
  theme_classic()

p_d_short_w_pred_err


ggsave(paste0(proj_root, "/img/modelest-vs-obs.pdf", width = 10, height = 5))

ggsave(plot = p_d_short_w_pred_err,
       filename = paste0(proj_root, "/img/modelest-vs-obs.png"),
                         width = 10, height = 5)
```






# Check posterior distribution of predictors

Let's have a look at the posterior distribution of the `best_model`.

```{r}
post_best_model <- extract.samples(models[[best_model_name]])
```


This object is a list. Let's convert it to a data frame for easier plotting.

```{r}
post_best_model_df <- tibble(
  sigma = post_best_model[["sigma"]],
  sigma2 = post_best_model[["sigma2"]],
  beta1 = post_best_model[["beta1"]],
  beta2= post_best_model[["beta2"]]
)
```

And now we plot a number of histograms:

```{r}
post_best_model_df <- post_best_model_df %>% 
  gather() %>%  
  rename(coef = key) %>% 
  mutate(coef = factor(coef, labels = c("beta1: foreigners rate", 
                                        "beta2: unemployment rate",
                                        "sigma: sd of outcome variable prior",
                                        "sigma2: sd of fedederal state prior")))
  
  
post_best_model_df %>% 
  ggplot() +
  aes(x = value) +
  facet_wrap(~coef, scales = "free") +
  theme_classic() + 
  geom_histogram() -> p_post_best_model

p_post_best_model
```


Now compute summary statistics:

```{r}
post_best_model_df %>% 
  group_by(coef) %>% 
  summarise(q05 = quantile(value, .05),
            q50 = quantile(value, .5),
            q95 = quantile(value, .95),
            value = mean(value)
  )  -> post_best_model_df_sum
  #gather(key = my_quantile, value = value, -coef) -> post_best_model_df_sum

head(post_best_model_df_sum)
```


Now plot both:

```{r}
p_post_best_model +
  geom_rect(data = post_best_model_df_sum,
              aes(xmin = q05,
                  xmax = q95,
                  ymin = 0,
                  ymax = Inf),
            fill = "red",
            alpha = .2) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y = element_blank()) +
  theme_classic() + 
  labs(
       y = "",
       caption = "Note. Shaded areas demark 90% mass intervals")

ggsave(paste0(proj_root, "/img/p-post-best-model.pdf"))
ggsave(paste0(proj_root, "/img/p-post-best-model.png"),
       width = 8, height = 4)
```








# Geo plotting


AfD success in the election:


```{r afd-geoplot}
wahlkreise_shp %>% 
  left_join(select(d, area_nr, afd_prop), by = c("WKR_NR" = "area_nr")) %>% 
  ggplot() +
  geom_sf(aes(fill = afd_prop)) +
  theme_void() +
  scale_fill_viridis() +
  labs(fill="Afd votes\n(Zweitstimme)",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_afd
p_afd

ggsave(paste0(proj_root, "/img/p-afd.pdf"))

ggsave(paste0(proj_root, "/img/p-afd.png"),
       width = 8, height = 4)

```


Unemployment rates in Germany per district:


```{r unemp-geoplot}
wahlkreise_shp %>% 
  left_join(select(d, area_nr, unemp_n, total_n), by = c("WKR_NR" = "area_nr")) %>% 
  mutate(unemp_prop = unemp_n / total_n) %>% 
  ggplot() +
  geom_sf(aes(fill = unemp_prop)) + 
  theme_void() +
  scale_fill_viridis() +
  labs(fill="unemployment rate",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_unemp
p_unemp

# 
ggsave(paste0(proj_root, "/img/p-unemp.pdf")
ggsave(paste0(proj_root, "/img/p-unemp.png"),
       width = 8, height = 4)       
```


Foreigner rates:

```{r for-geoplot}
wahlkreise_shp %>% 
  left_join(select(d, area_nr, for_prop, total_n), by = c("WKR_NR" = "area_nr")) %>% 
  ggplot() +
  geom_sf(aes(fill = for_prop)) + 
  theme_void() +
  scale_fill_viridis() +
  labs(fill="Foreigner rate",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_foreign

p_foreign

# ggsave(paste0(proj_root, "/img/p-foreigners.pdf")
ggsave(paste0(proj_root, "/img/p-foreign.png"),
       width = 8, height = 4)
```


Joint diagrams

```{r}
library(gridExtra)

grid.arrange(p_unemp, p_afd, nrow = 1)
grid.arrange(p_foreign, p_afd, nrow = 1)
```





## Plot prediction errors



```{r pred-error-geoplot}
wahlkreise_shp %>% 
  left_join(select(d79, area_nr, err_avg), by = c("WKR_NR" = "area_nr")) %>% 
  #drop_na(afd_prop) %>% 
  ggplot() +
  geom_sf(aes(fill = err_avg)) + 
  theme_void() +
  scale_fill_viridis() +
  labs(title = "Average mean prediction errors of best model",
       fill = "Average absolute \nprediction error",
       caption = "Data provided by the Bundeswahlleiter 2017") -> p_pred_error

p_pred_error

ggsave(paste0(proj_root, "/img/p_pred_error.pdf"))
ggsave(plot = p_pred_error,
       filename = paste0(proj_root, "/img/p_pred_error.png"),
       width = 5, height = 5)
```











# Check linearity assumption



Let's compute the predictions for each model:



```{r fun-comp-preds}

model_predictions <- lapply(models79, link)
 
```


Now plot the predictions against the error, as advised by Gelman and Hill.

First, get predictions of the best moel:

```{r}
model_predictions[[best_model_name]] %>% 
  as_tibble %>% 
  summarise_all(mean) %>% 
  gather() -> best_model_preds

```

Each observation is one *row*  in this data frame.

Similarly, get errors of the best model:


```{r}
model_error[[best_model_name]] %>% 
  as_tibble() %>% 
  rename(error = value) %>% 
  mutate(pred = best_model_preds$value) -> best_model_pred_err
p_best_model_pred_err <- best_model_pred_err %>% 
  ggplot() +
  aes(x = pred, y = error) +
  geom_hline(yintercept = quantile(best_model_pred_err$error, .5),
             color = "grey60") +
  geom_hline(yintercept = quantile(best_model_pred_err$error, .975),
             color = "grey80", , linetype = "dashed") +
  geom_hline(yintercept = quantile(best_model_pred_err$error, .025),
             color = "grey80", , linetype = "dashed") +
  geom_point() +
  theme_classic() +
  labs(title = best_model_name,
       xlab = "Model predictions",
       ylab = "Model error",
       caption = "Note. Horizontal lines denote .025, .5, and .975 quantiles, respectively") 

p_best_model_pred_err 

 
ggsave(p_best_model_pred_err ,
       paste0(proj_root, "/img/p_best_model_pred_err.pdf"))
ggsave(plot = p_best_model_pred_err ,
       filename = paste0(proj_root, "/img/p_best_model_pred_err.png"), width = 8, height = 4)
```





# SessionInfo

```{r}
sessionInfo()
```







# Save results


```{r eval = params$readonly}
save(models, 
     file = paste0(proj_root, "/objects/models.Rda"))

save(compare_null_models,
     file = paste0(proj_root, "/objects/compare_null_models.Rda"))

save(compare_reduced_sample,
     file = paste0(proj_root, "/objects/compare_reduced_sample.Rda"))

save(model_error, 
     file = paste0(proj_root, "/objects/model_error.Rda"))

save(modell_error_IQR, 
     file = paste0(proj_root, "/objects/modell_error_IQR.Rda"))


save(model_error_df, 
     file = paste0(proj_root, "/objects/model_error_df.RDa"))

save(model_error_IQR_df, 
     file = paste0(proj_root, "/objects/model_error_IQR_df.RDa"))


save(model_error_md_iqr, 
     file = paste0(proj_root, "/objects/model_error_md_iqr.RDa"))


save(best_model_preds, 
     file = paste0(proj_root, "/objects/best_model_preds.Rda"))

save(model_predictions,
     file = paste0(proj_root, "/objectsmodel_predictions.Rda"))


save(model_avg_predictions,
     file = paste0(proj_root, "/model_avg_predictions.Rda"))


```

